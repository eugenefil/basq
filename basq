#!/usr/bin/env winpython3

import sys
import csv
import argparse
import datetime
import decimal

import adodbapi
import pyodbc


COLUMN_NAME = 0
COLUMN_TYPE = 1
COLUMN_SCALE = 5

CONN_STR_MSSQL = u'Provider=SQLNCLI10;Server={server};Database={database};{auth}'
CONN_STR_MSSQL_SRVAUTH = u'Uid={user};Pwd={password}'
CONN_STR_MSSQL_WINAUTH = u'Integrated Security=SSPI'


def execsql(cursor, cmd, input_rows=None, typeconv=None):
    if not input_rows is None:
        cursor.executemany(cmd, input_rows)
    else:
        cursor.execute(cmd)

    # don't return any results if there are no
    if cursor.description is None:
        return None

    columns = []
    for i, desc in enumerate(cursor.description):
        coltype = typeconv(desc[COLUMN_TYPE])
        columns.append({'name': desc[COLUMN_NAME], 'type': coltype})

    return cursor, columns


def todate(s):
    return datetime.datetime.strptime(s, '%Y-%m-%d').date()

def totime(s):
    return datetime.datetime.strptime(s, '%H:%M:%S').time()

def type_parsers():
    idfunc = lambda x: x
    return {
        'string': idfunc,
        'number': float,
        # VFP-SPECIFIC: Pass integers to vfp provider as floats! There
        # seems to be a bug in vfp oledb provider which results in
        # integers passed incorrectly. With python adodbapi integers
        # get passed as zeroes. Passing integers in vbscript with
        # ADODB.Command I got: 1130168319 for passed -1, 1130102784
        # for 0, 1130102785 for 1 and so on. In contrast passing
        # integers to Microsoft ACE provider works fine.
        'integer': float,
        'date': todate,
        'time': totime,
        'boolean': lambda x: bool(int(x))
    }


def tolist(row, colnames): return row
def todict(row, colnames): return dict(zip(colnames, row))

def rowfunc(paramstyle):
    return {
        'qmark': tolist,
        'named': todict
    }.get(paramstyle, tolist)


def readrows(file, type_parsers, rowfunc=tolist):
    """Parse input CSV rows from file and return rows of values.

    Parse each value in a input row from its string representation via
    corresponding parser from type_parsers. type_parsers is a dict,
    where types are keys and their parsers are values. If column misses
    type, string is assumed.

    Convert row of values to output row format (list or dict) with
    rowfunc.
    """
    reader = csv.reader(file)

    header = next(reader)
    header = [
        dict(zip(
            ('name', 'type'),
            # assume string type if missing
            col.split(' ') + ['string']
        ))
        for col in header
    ]

    colparsers = [type_parsers[col['type']] for col in header]
    colnames = [col['name'] for col in header]

    rows = []
    for row in reader:
        # stop reading on empty line
        if row == []:
            break

        rows.append(rowfunc(
            # pyodbc breaks on map objects, use lists
            list(map(lambda f, v: f(v), colparsers, row)),
            colnames
        ))
    return rows


def writerows(file, rows, cols, typed_header=False):
    """Write output rows to file in CSV format.

    If typed_header is True, output column type after column name
    delimited with space.
    """
    writer = csv.writer(file)

    colnamefunc = {
        False: lambda col: col['name'],
        True: lambda col: col['name'] + ' ' + col['type']
    }[typed_header]
    writer.writerow(colnamefunc(c) for c in cols)

    writer.writerows(rows)


def main(
        conn,
        paramstyle=None,
        typed_header=False,
        autocommit=False,
        typeconv=None
    ):
    cur = conn.cursor()

    for line in sys.stdin:
        sqlcmd = line.strip()
        if not sqlcmd:
            continue

        input_rows = None
        if paramstyle:
            input_rows = readrows(
                sys.stdin,
                type_parsers(),
                rowfunc(paramstyle)
            )

        results = execsql(
            cur,
            sqlcmd,
            input_rows=input_rows,
            typeconv=typeconv
        )
        if results:
            rows, cols = results
            writerows(
                sys.stdout,
                rows,
                cols,
                typed_header=typed_header
            )

    cur.close()
    if not autocommit:
        conn.commit()
    conn.close()


def cvtDate(default_conv):
    """Return datetime.date from COM date.

    Default adodbapi converter converts COM date (which is number of
    days since 1899-12-30) to datetime.datetime. This converter extracts
    date from that datetime.
    """
    def todate(com_date):
        return default_conv(com_date).date()
    return todate


def cvtRTrim(s):
    return s.rstrip(' ')


def vfpconn(args):
    connstr = u'Provider=VFPOLEDB.1;Data Source={datasource};Mode=Share Deny None;Extended Properties="";User ID="";Mask Password=False;Cache Authentication=False;Encrypt Password=False;Collating Sequence=MACHINE;DSN="";DELETED=True;CODEPAGE=1251;MVCOUNT=16384;ENGINEBEHAVIOR=90;TABLEVALIDATE=3;REFRESH=5;VARCHARMAPPING=False;ANSI=True;REPROCESS=5'
    connstr = connstr.format(datasource=args.database)
    conn = adodbapi.connect(connstr, autocommit=args.autocommit)

    # set proper connection-wide db-to-python type conversions
    conn.variantConversions = adodbapi.apibase.variantConversions
    # convert numeric to float
    conn.variantConversions[adodbapi.ado_consts.adNumeric] = (
        adodbapi.apibase.cvtFloat
    )
    # fixed-width char columns in FoxPro are returned with trailing
    # spaces, so right-trimming is necessary
    conn.variantConversions[adodbapi.ado_consts.adChar] = cvtRTrim
    # convert pure date to date, not datetime
    conn.variantConversions[adodbapi.ado_consts.adDBDate] = cvtDate(
        conn.variantConversions[adodbapi.ado_consts.adDBDate]
    )

    if args.paramstyle:
        conn.paramstyle = args.paramstyle

    return conn


ADO_INTEGER = adodbapi.apibase.DBAPITypeObject(
    adodbapi.apibase.adoIntegerTypes +
    adodbapi.apibase.adoLongTypes
)

def ado_type_to_general(type):
    if type == adodbapi.STRING:
        return 'string'
    # compare before int, since bool also compares true to int types
    elif type == adodbapi.ado_consts.adBoolean:
        return 'boolean'
    elif type == ADO_INTEGER:
        return 'integer'
    elif type == adodbapi.NUMBER:
        return 'number'
    elif type == adodbapi.DATETIME:
        return 'date'
    raise TypeError('ADO type %s cannot be converted to general type' % type)


def vfpvals(*args, **kwargs):
    return {
        'conn': vfpconn(*args, **kwargs),
        'typeconv': ado_type_to_general
    }


def odbc_type_to_general(type):
    # check number types explicitly, because pyodbc.NUMBER == float
    if type == int:
        return 'integer'
    elif type == decimal.Decimal:
        return 'number'
    elif type == pyodbc.STRING:
        return 'string'
    elif type == datetime.date:
        return 'date'
    elif type == datetime.time:
        return 'time'
    raise TypeError('ODBC type %s cannot be converted to general type' % type)


def tpsvals(args):
    connstr = 'DRIVER={{Topspeed ODBC Driver}};DBQ={datasource}\!;Extension=tps;Oem=N;Datefield={datefields};Timefield={timefields};SERVER=NotTheServer;'
    connstr = connstr.format(
        datasource=args.database,
        datefields=args.datefields.replace(',', '|'),
        timefields=args.timefields.replace(',', '|')
    )
    # pyodbc disables autocommit by default making TopSpeed fail with
    # SQLSetConnnectAttr(SQL_ATTR_AUTOCOMMIT), because it does not
    # support autocommit. We enable it, so pyodbc does not try to
    # disable it.
    conn = pyodbc.connect(connstr, autocommit=True)

    if args.paramstyle and args.paramstyle != 'qmark':
        raise Exception('paramstyle other than qmark is not supported')

    return {
        'conn': conn,
        'typeconv': odbc_type_to_general
    }


def mssqlconnstr(args):
    auth = CONN_STR_MSSQL_WINAUTH
    if args.user:
        auth = CONN_STR_MSSQL_SRVAUTH.format(
            user=args.user,
            password=args.password
        )
    return CONN_STR_MSSQL.format(
        auth=auth,
        server=args.server,
        database=args.database
    )


def parse_args():
    p = argparse.ArgumentParser()
    p.add_argument(
        '--paramstyle',
        choices=('qmark', 'named'),
        help='execute parameterized query. Input values are read as CSV (with typed header assumed) that follows right after the line of query on stdin. Param style in query is one of: qmark (e.g. where id = ?) or named (e.g. where id = :id). With latter column id must be in CSV. Default style: qmark'
    )
    p.add_argument(
        '--typed-header',
        action='store_true',
        help='output typed header. Each column will contain its type delimited from name by space'
    )
    p.add_argument(
        '--autocommit',
        action='store_true',
        help='commit after each SQL statement. By default, SQL statements are executed inside a transaction. E.g. FoxPro DDL statements work only in this mode'
    )

    subparsers = p.add_subparsers(title='commands')

    vfp = subparsers.add_parser(
        'vfp',
        help='Visual FoxPro'
    )
    vfp.add_argument(
        'database',
        help='path to .dbc database file'
    )
    vfp.set_defaults(getdbvals=vfpvals)

    tps = subparsers.add_parser(
        'tps',
        help='TopSpeed'
    )
    tps.add_argument(
        '--datefields',
        default='',
        help='comma-separated list of date fields (not aliases!) in query'
    )
    tps.add_argument(
        '--timefields',
        default='',
        help='comma-separated list of time fields (not aliases!) in query'
    )
    tps.add_argument(
        'database',
        help='path to .tps database file'
    )
    tps.set_defaults(getdbvals=tpsvals)

    # mssqlparser = subparsers.add_parser(
    #     'mssql',
    #     description='Windows Authentication is used by default',
    #     help='Microsoft SQL Server'
    # )
    # mssqlparser.add_argument(
    #     '-server',
    #     default='localhost',
    #     help='server ip/name. Default: localhost',
    #     metavar='SRV'
    # )
    # mssqlparser.add_argument(
    #     '-database',
    #     default='',
    #     help='initial database. Default: server settings',
    #     metavar='DB'
    # )
    # mssqlparser.add_argument(
    #     '-user',
    #     help='user name for SQL Server Authentication'
    # )
    # mssqlparser.add_argument(
    #     '-password',
    #     default='',
    #     help='password for SQL Server Authentication',
    #     metavar='PASS'
    # )
    # mssqlparser.set_defaults(getconnstr=mssqlconnstr)

    args = p.parse_args()
    # exit if no database was chosen
    if not 'getdbvals' in args:
        p.print_help()
        sys.exit(1)
    return args


def setup():
    # Redefine stdin to not translate newlines. Otherwise when reading
    # CSV field containing \r\n on Windows it gets translated to \n,
    # i.e. data gets corrupted. Always use utf-8.
    sys.stdin = open(
        sys.stdin.fileno(),
        mode=sys.stdin.mode,
        encoding='utf-8',
        errors=sys.stdin.errors,
        newline='',
        closefd=False
    )
    # Redefine stdout to not translate newlines. CSV module (as per
    # rfc 4180) writes \r\n. Otherwise when on Windows, \n is
    # translated to \r\n, so original \r\n becomes \r\r\n. Always use
    # utf-8.
    sys.stdout = open(
        sys.stdout.fileno(),
        mode=sys.stdout.mode,
        encoding='utf-8',
        errors=sys.stdout.errors,
        newline='',
        closefd=False
    )


if __name__ == '__main__':
    args = parse_args()
    setup()
    dbvals = args.getdbvals(args)
    main(
        dbvals['conn'],
        paramstyle=args.paramstyle,
        typed_header=args.typed_header,
        autocommit=args.autocommit,
        typeconv=dbvals['typeconv']
    )
